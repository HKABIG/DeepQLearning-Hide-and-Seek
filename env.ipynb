{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it goes to the yellow to end the game\n",
    "it keeps doing +1 -1 to stay in its place without hitting the wall\n",
    "\n",
    "ideas:\n",
    "maybe reduce the wall neg rew to -10\n",
    "increase prob based on eps or step\n",
    "maybe increase reward for distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m tensorboard.main --logdir=./src/logs/DQN_36\n",
    "\n",
    "from typing import Optional, Union\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from utils.vision import compute_visible_cells\n",
    "from helpers import default_map\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pathfinding.core.diagonal_movement import DiagonalMovement\n",
    "from pathfinding.core.grid import Grid\n",
    "from pathfinding.finder.a_star import AStarFinder\n",
    "from pathfinding.finder.dijkstra import DijkstraFinder\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import random\n",
    "\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (0, 0, 255)\n",
    "BLUE = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "YELLOW = (0, 255, 255)\n",
    "\n",
    "colors = np.array([WHITE,  # hidden cells\n",
    "                   YELLOW, # visible cells\n",
    "                   BLACK,  # walls\n",
    "                   RED,  # seeker\n",
    "                   GREEN],   # hider\n",
    "                  dtype=np.uint8)\n",
    "\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "STAY = 4\n",
    "\n",
    "HITTING_WALL_REWARD = -20\n",
    "LOSE_REWARD = -100\n",
    "DISTANCE_COEF_REWARD = 1\n",
    "\n",
    "OBS_DICT = {\n",
    "            \"hidden\":0,\n",
    "            \"visible\":1,\n",
    "            \"wall\":2,\n",
    "            \"seeker\":3,\n",
    "            \"hider\":4\n",
    "        }\n",
    "\n",
    "MIN_RES = 36\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HideAndSeekEnv(gym.Env):\n",
    "    def __init__(self, \n",
    "                 observation_handler: ObservationHandler,\n",
    "                 grid_size=12, \n",
    "                 vision_range=5, \n",
    "                 seq_len=5,\n",
    "                 use_cache=True, \n",
    "                 max_steps=None,\n",
    "                 render_mode='rgb_array',\n",
    "                 fps=5,\n",
    "                ):\n",
    "        super(HideAndSeekEnv, self).__init__()\n",
    "\n",
    "        assert vision_range > 0, \"Vision range must be greater than 0\"\n",
    "        assert grid_size > 0, \"Grid size must be greater than 0\"\n",
    "        assert max_steps is None or max_steps > 0, \"Max steps must be greater than 0\" \n",
    "        assert render_mode in ['rgb_array', 'human'], \"Render mode must be 'rgb_array' or 'human'\"\n",
    "        assert seq_len > 0, \"Sequence length must be greater than 0\"\n",
    "        \n",
    "        if max_steps is None:\n",
    "            max_steps = np.inf\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.vision_range = vision_range\n",
    "        self.seq_len = seq_len\n",
    "        self.render_mode = render_mode\n",
    "        self.use_cache = use_cache\n",
    "        self.fps = fps\n",
    "\n",
    "        self.seeker_pos = None\n",
    "        self.hider_pos = None\n",
    "        self.cache = {\n",
    "            'best_seeker_action':-1*np.ones(\n",
    "                (self.grid_size, self.grid_size, self.grid_size, self.grid_size), \n",
    "                dtype=int,\n",
    "            ),\n",
    "            'visible_cells':{},\n",
    "        }\n",
    "        self.upscale_factor = math.ceil(MIN_RES/self.grid_size)\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right, Stay\n",
    "        self.obs_dict = OBS_DICT\n",
    "        self.observation_space = observation_handler.get_observation_space()\n",
    "        self.observation_handler = observation_handler\n",
    "\n",
    "        self.current_state = None\n",
    "        self.current_step = 0\n",
    "        self.prev_states = None\n",
    "        self.walls = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "        self.visible_cells = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "\n",
    "        self.current_eps = 0\n",
    "        self.mode = None\n",
    "        self.train()\n",
    "        check_env(self)\n",
    "        self.current_eps = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def eval(self):\n",
    "        self.mode = 'eval'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.observation_handler.get_observation(\n",
    "            walls=self.walls, \n",
    "            visible_cells=self.visible_cells, \n",
    "            seeker_pos=self.seeker_pos, \n",
    "            hider_pos=self.hider_pos\n",
    "        )\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None,):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.prev_states = np.empty((self.grid_size, self.grid_size, 0), dtype=np.uint8)\n",
    "\n",
    "        self.walls = self._generate_walls()\n",
    "        self.seeker_pos = self.generate_seeker_pos()\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "        self.hider_pos = self.generate_hider_pos()\n",
    "\n",
    "        return self.get_state(), self._get_info()\n",
    "\n",
    "    def get_visible_cells(self):\n",
    "        cache = self.cache['visible_cells']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1])\n",
    "            if key in cache:\n",
    "                return cache[key]\n",
    "\n",
    "        visible_cells = compute_visible_cells(self.walls, self.seeker_pos, self.vision_range)\n",
    "        if self.use_cache:\n",
    "            cache[key] = visible_cells\n",
    "\n",
    "        return visible_cells\n",
    "\n",
    "    def _generate_walls(self):\n",
    "        walls_mask = default_map()\n",
    "        return walls_mask\n",
    "        \n",
    "    def generate_hider_pos(self):\n",
    "        allowed_cells = ~(self.walls | self.visible_cells)\n",
    "        allowed_cells[self.seeker_pos[0], self.seeker_pos[1]] = False\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "\n",
    "    def generate_seeker_pos(self):\n",
    "        allowed_cells = ~self.walls\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "    \n",
    "    def index_to_coords(self, index):\n",
    "        return index // self.grid_size, index % self.grid_size\n",
    "    \n",
    "    def coords_to_index(self, coords):\n",
    "        return coords[0] * self.grid_size + coords[1]\n",
    "    \n",
    "    def sample_from_allowed_cells(self, allowed_cells):\n",
    "        prob = allowed_cells.astype(np.float32)\n",
    "        prob /= prob.sum()\n",
    "        flat_prob = prob.flatten()\n",
    "        sample_index = np.random.choice(flat_prob.size, p=flat_prob)\n",
    "        x, y = self.index_to_coords(sample_index)\n",
    "        return np.array([x, y], dtype=np.uint8)\n",
    "    \n",
    "    def _get_valid_actions(self, position):\n",
    "        valid_actions = []\n",
    "        if position[0] > 0 and not self.walls[position[0] - 1, position[1]]:\n",
    "            valid_actions.append(UP)\n",
    "        if position[0] < self.grid_size - 1 and not self.walls[position[0] + 1, position[1]]:\n",
    "            valid_actions.append(DOWN)\n",
    "        if position[1] > 0 and not self.walls[position[0], position[1] - 1]:\n",
    "            valid_actions.append(LEFT)\n",
    "        if position[1] < self.grid_size - 1 and not self.walls[position[0], position[1] + 1]:\n",
    "            valid_actions.append(RIGHT)\n",
    "        # valid_actions.append(STAY)\n",
    "        return valid_actions\n",
    "    \n",
    "    def _move(self, position, action):\n",
    "        if action == UP:  # Up\n",
    "            position[0] -= 1\n",
    "        elif action == DOWN:  # Down\n",
    "            position[0] += 1\n",
    "        elif action == LEFT:  # Left\n",
    "            position[1] -= 1\n",
    "        elif action == RIGHT:  # Right\n",
    "            position[1] += 1\n",
    "        elif action == STAY:  # Stay\n",
    "            print(\"Agent action: STAY ?!\")\n",
    "            pass\n",
    "        return position\n",
    "    \n",
    "    def _get_min_distance_from_visible_cells(self, position):\n",
    "        distances = np.linalg.norm(np.indices((self.grid_size, self.grid_size)) - position[:, np.newaxis, np.newaxis], axis=0)\n",
    "        distances[~self.visible_cells] = np.inf\n",
    "        return distances.min()\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "    \n",
    "    def move_player(self, action):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "        assert self.mode in ['test'], \"Call move_player only in test mode\"\n",
    "\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "    \n",
    "    def step(self, action, verbose=False):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = 0\n",
    "        reward_log = {}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # seeker makes a move based on the old state\n",
    "            self._move_seeker()\n",
    "\n",
    "        valid_actions = self._get_valid_actions(self.hider_pos)\n",
    "        if action in valid_actions:\n",
    "            self._move(self.hider_pos, action)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Hider hit the wall\")\n",
    "            reward += HITTING_WALL_REWARD\n",
    "            reward_log['hitting_wall'] = HITTING_WALL_REWARD\n",
    "\n",
    "        min_distance = self._get_min_distance_from_visible_cells(self.hider_pos)\n",
    "        distance_reward = int(min_distance) * DISTANCE_COEF_REWARD\n",
    "        reward += distance_reward\n",
    "        reward_log['distance'] = distance_reward\n",
    "\n",
    "        terminated = False\n",
    "        if self.visible_cells[self.hider_pos[0], self.hider_pos[1]]:\n",
    "            if verbose:\n",
    "                print(\"Hider was caught\")\n",
    "            reward += LOSE_REWARD\n",
    "            reward_log['lose'] = LOSE_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        truncated = self.current_step >= self.max_steps and not terminated\n",
    "\n",
    "        if terminated or truncated:\n",
    "            self.current_eps += 1\n",
    "\n",
    "        return self.get_state(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def _generate_frame(self, matrix, cell_size=50):\n",
    "        # Calculate image size based on grid dimensions and cell size\n",
    "        image_size = (matrix.shape[1] * cell_size, matrix.shape[0] * cell_size)\n",
    "\n",
    "        # Create a blank canvas with white background\n",
    "        image = np.ones((matrix.shape[1], matrix.shape[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Fill each cell with the corresponding color using NumPy indexing\n",
    "        image_rows = np.arange(matrix.shape[0]) * cell_size\n",
    "        image_cols = np.arange(matrix.shape[1]) * cell_size\n",
    "\n",
    "        # print(image[0, :60])\n",
    "        image = colors[matrix]\n",
    "        # repeat each row cell_size times\n",
    "        image = np.repeat(image, cell_size, axis=0)\n",
    "        image = np.repeat(image, cell_size, axis=1)\n",
    "\n",
    "        # Draw black lines as separators between cells using NumPy indexing\n",
    "        image[::cell_size, :] = (0, 0, 0)\n",
    "        image[:, ::cell_size] = (0, 0, 0)\n",
    "        image[::cell_size, -1] = (0, 0, 0)\n",
    "        image[-1, ::cell_size] = (0, 0, 0)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            frame = self._generate_frame(self.current_state)\n",
    "            cv2.imshow('Hide & Seek', frame)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            print(self.current_state)\n",
    "        \n",
    "    def get_best_seeker_action(self):\n",
    "        cache = self.cache['best_seeker_action']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1], self.hider_pos[0], self.hider_pos[1])\n",
    "            if cache[key]!=-1:\n",
    "                return cache[key]\n",
    "\n",
    "        best_action = self._compute_best_seeker_action(self.walls, self.seeker_pos, self.hider_pos)\n",
    "        if self.use_cache:\n",
    "            cache[key] = best_action\n",
    "\n",
    "        return best_action\n",
    "    \n",
    "    def _compute_best_seeker_action(self, walls, seeker_pos, hider_pos):\n",
    "        grid = Grid(matrix=~walls)\n",
    "        start = grid.node(seeker_pos[1], seeker_pos[0])\n",
    "        end = grid.node(hider_pos[1], hider_pos[0])\n",
    "        finder = DijkstraFinder()\n",
    "        path, runs = finder.find_path(start, end, grid)\n",
    "        if len(path) == 0: # no path found\n",
    "            return STAY\n",
    "        next_cell = path[-len(path)+1]\n",
    "        best_move = (next_cell[1] - seeker_pos[0], next_cell[0] - seeker_pos[1])\n",
    "        best_action = self._move_to_action(best_move)\n",
    "        if best_action == STAY:\n",
    "            print(\"Seeker best action is to stay ?!\")\n",
    "        return best_action\n",
    "\n",
    "    def _move_to_action(self, move):\n",
    "        if move == (-1, 0):\n",
    "            action = UP\n",
    "        elif move == (1, 0):\n",
    "            action = DOWN\n",
    "        elif move == (0, -1):\n",
    "            action = LEFT\n",
    "        elif move == (0, 1):\n",
    "            action = RIGHT\n",
    "        elif move == (0, 0):\n",
    "            action = STAY\n",
    "        return action\n",
    "\n",
    "    def _move_seeker(self):\n",
    "        # best_action = self.get_best_seeker_action()\n",
    "        # prob = self.current_step / self.max_steps\n",
    "        prob = 0.2\n",
    "        # if self.current_step < 10:\n",
    "        #     return\n",
    "        if np.random.binomial(1, prob):\n",
    "            action = self.get_best_seeker_action()\n",
    "        else:\n",
    "            valid_actions = self._get_valid_actions(self.seeker_pos)\n",
    "            action = random.choice(valid_actions)\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "\n",
    "        \n",
    "env = Monitor(HideAndSeekEnv(\n",
    "    observation_handler=MLPObservationHandler(grid_size=12),\n",
    "    grid_size=12,\n",
    "    vision_range=5,\n",
    "    seq_len=1,\n",
    "    render_mode=\"human\",\n",
    "    use_cache=True,\n",
    "    max_steps=300,\n",
    "    # start_movement_at=100_000,\n",
    "    # start_opt_movement_at=10,\n",
    "))\n",
    "# env.render()\n",
    "# env.step(UP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HideAndSeekEnv(gym.Env):\n",
    "    def __init__(self, \n",
    "                 grid_size=12, \n",
    "                 vision_range=5, \n",
    "                 seq_len=5,\n",
    "                 use_cache=True, \n",
    "                 max_steps=None,\n",
    "                 render_mode='rgb_array',\n",
    "                 fps=5,\n",
    "                ):\n",
    "        super(HideAndSeekEnv, self).__init__()\n",
    "\n",
    "        assert vision_range > 0, \"Vision range must be greater than 0\"\n",
    "        assert grid_size > 0, \"Grid size must be greater than 0\"\n",
    "        assert max_steps is None or max_steps > 0, \"Max steps must be greater than 0\" \n",
    "        assert render_mode in ['rgb_array', 'human'], \"Render mode must be 'rgb_array' or 'human'\"\n",
    "        assert seq_len > 0, \"Sequence length must be greater than 0\"\n",
    "        \n",
    "        if max_steps is None:\n",
    "            max_steps = np.inf\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.vision_range = vision_range\n",
    "        self.seq_len = seq_len\n",
    "        self.render_mode = render_mode\n",
    "        self.use_cache = use_cache\n",
    "        self.fps = fps\n",
    "\n",
    "        self.seeker_pos = None\n",
    "        self.hider_pos = None\n",
    "        self.cache = {\n",
    "            'best_seeker_action':-1*np.ones(\n",
    "                (self.grid_size, self.grid_size, self.grid_size, self.grid_size), \n",
    "                dtype=int,\n",
    "            ),\n",
    "            'visible_cells':{},\n",
    "        }\n",
    "        self.upscale_factor = math.ceil(MIN_RES/self.grid_size)\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right, Stay\n",
    "        self.obs_dict = {\n",
    "            \"hidden\":0,\n",
    "            \"visible\":1,\n",
    "            \"wall\":2,\n",
    "            \"seeker\":3,\n",
    "            \"hider\":4\n",
    "        }\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=len(self.obs_dict)-1,\n",
    "            shape=(self.grid_size*self.grid_size, ),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        self.current_state = None\n",
    "        self.current_step = 0\n",
    "        self.prev_states = None\n",
    "        self.walls = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "        self.visible_cells = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "\n",
    "        self.current_eps = 0\n",
    "        self.mode = None\n",
    "        self.train()\n",
    "        check_env(self)\n",
    "        self.current_eps = 0\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def eval(self):\n",
    "        self.mode = 'eval'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def get_processed_state(self):\n",
    "        return self.current_state.flatten()\n",
    "\n",
    "    def update_current_state(self):\n",
    "        self.current_state = np.full(\n",
    "            shape=(self.grid_size, self.grid_size), \n",
    "            fill_value=self.obs_dict['hidden'], \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.current_state[self.walls] = self.obs_dict['wall']\n",
    "        self.current_state[self.visible_cells] = self.obs_dict['visible']\n",
    "        self.current_state[self.seeker_pos[0], self.seeker_pos[1]] = self.obs_dict['seeker']\n",
    "        self.current_state[self.hider_pos[0], self.hider_pos[1]] = self.obs_dict['hider']\n",
    "        \n",
    "        # self.prev_states = np.append(\n",
    "        #     self.prev_states, \n",
    "        #     np.expand_dims(self.current_state, axis=-1), \n",
    "        #     axis=-1\n",
    "        # )\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None,):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.prev_states = np.empty((self.grid_size, self.grid_size, 0), dtype=np.uint8)\n",
    "\n",
    "        self.walls = self._generate_walls()\n",
    "        self.seeker_pos = self.generate_seeker_pos()\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "        self.hider_pos = self.generate_hider_pos()\n",
    "\n",
    "        for _ in range(self.seq_len):\n",
    "            self.update_current_state()\n",
    "\n",
    "        return self.get_processed_state(), self._get_info()\n",
    "\n",
    "    def get_visible_cells(self):\n",
    "        cache = self.cache['visible_cells']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1])\n",
    "            if key in cache:\n",
    "                return cache[key]\n",
    "\n",
    "        visible_cells = compute_visible_cells(self.walls, self.seeker_pos, self.vision_range)\n",
    "        if self.use_cache:\n",
    "            cache[key] = visible_cells\n",
    "\n",
    "        return visible_cells\n",
    "\n",
    "    def _generate_walls(self):\n",
    "        walls_mask = default_map()\n",
    "        return walls_mask\n",
    "        \n",
    "    def generate_hider_pos(self):\n",
    "        allowed_cells = ~(self.walls | self.visible_cells)\n",
    "        allowed_cells[self.seeker_pos[0], self.seeker_pos[1]] = False\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "\n",
    "    def generate_seeker_pos(self):\n",
    "        allowed_cells = ~self.walls\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "    \n",
    "    def index_to_coords(self, index):\n",
    "        return index // self.grid_size, index % self.grid_size\n",
    "    \n",
    "    def coords_to_index(self, coords):\n",
    "        return coords[0] * self.grid_size + coords[1]\n",
    "    \n",
    "    def sample_from_allowed_cells(self, allowed_cells):\n",
    "        prob = allowed_cells.astype(np.float32)\n",
    "        prob /= prob.sum()\n",
    "        flat_prob = prob.flatten()\n",
    "        sample_index = np.random.choice(flat_prob.size, p=flat_prob)\n",
    "        x, y = self.index_to_coords(sample_index)\n",
    "        return np.array([x, y], dtype=np.uint8)\n",
    "    \n",
    "    def _get_valid_actions(self, position):\n",
    "        valid_actions = []\n",
    "        if position[0] > 0 and not self.walls[position[0] - 1, position[1]]:\n",
    "            valid_actions.append(UP)\n",
    "        if position[0] < self.grid_size - 1 and not self.walls[position[0] + 1, position[1]]:\n",
    "            valid_actions.append(DOWN)\n",
    "        if position[1] > 0 and not self.walls[position[0], position[1] - 1]:\n",
    "            valid_actions.append(LEFT)\n",
    "        if position[1] < self.grid_size - 1 and not self.walls[position[0], position[1] + 1]:\n",
    "            valid_actions.append(RIGHT)\n",
    "        # valid_actions.append(STAY)\n",
    "        return valid_actions\n",
    "    \n",
    "    def _move(self, position, action):\n",
    "        if action == UP:  # Up\n",
    "            position[0] -= 1\n",
    "        elif action == DOWN:  # Down\n",
    "            position[0] += 1\n",
    "        elif action == LEFT:  # Left\n",
    "            position[1] -= 1\n",
    "        elif action == RIGHT:  # Right\n",
    "            position[1] += 1\n",
    "        elif action == STAY:  # Stay\n",
    "            print(\"Agent action: STAY ?!\")\n",
    "            pass\n",
    "        return position\n",
    "    \n",
    "    def _get_min_distance_from_visible_cells(self, position):\n",
    "        distances = np.linalg.norm(np.indices((self.grid_size, self.grid_size)) - position[:, np.newaxis, np.newaxis], axis=0)\n",
    "        distances[~self.visible_cells] = np.inf\n",
    "        return distances.min()\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "    \n",
    "    def move_player(self, action):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "        assert self.mode in ['test'], \"Call move_player only in test mode\"\n",
    "\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "    \n",
    "    def step(self, action, verbose=False):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "        assert self.current_state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = 0\n",
    "        reward_log = {}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # seeker makes a move based on the old state\n",
    "            self._move_seeker()\n",
    "\n",
    "        valid_actions = self._get_valid_actions(self.hider_pos)\n",
    "        if action in valid_actions:\n",
    "            self._move(self.hider_pos, action)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Hider hit the wall\")\n",
    "            reward += HITTING_WALL_REWARD\n",
    "            reward_log['hitting_wall'] = HITTING_WALL_REWARD\n",
    "\n",
    "        min_distance = self._get_min_distance_from_visible_cells(self.hider_pos)\n",
    "        distance_reward = int(min_distance) * DISTANCE_COEF_REWARD\n",
    "        reward += distance_reward\n",
    "        reward_log['distance'] = distance_reward\n",
    "\n",
    "        terminated = False\n",
    "        if self.visible_cells[self.hider_pos[0], self.hider_pos[1]]:\n",
    "            if verbose:\n",
    "                print(\"Hider was caught\")\n",
    "            reward += LOSE_REWARD\n",
    "            reward_log['lose'] = LOSE_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        truncated = self.current_step >= self.max_steps and not terminated\n",
    "        self.update_current_state()\n",
    "\n",
    "        if terminated or truncated:\n",
    "            self.current_eps += 1\n",
    "\n",
    "        return self.get_processed_state(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def _generate_frame(self, matrix, cell_size=50):\n",
    "        # Calculate image size based on grid dimensions and cell size\n",
    "        image_size = (matrix.shape[1] * cell_size, matrix.shape[0] * cell_size)\n",
    "\n",
    "        # Create a blank canvas with white background\n",
    "        image = np.ones((matrix.shape[1], matrix.shape[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Fill each cell with the corresponding color using NumPy indexing\n",
    "        image_rows = np.arange(matrix.shape[0]) * cell_size\n",
    "        image_cols = np.arange(matrix.shape[1]) * cell_size\n",
    "\n",
    "        # print(image[0, :60])\n",
    "        image = colors[matrix]\n",
    "        # repeat each row cell_size times\n",
    "        image = np.repeat(image, cell_size, axis=0)\n",
    "        image = np.repeat(image, cell_size, axis=1)\n",
    "\n",
    "        # Draw black lines as separators between cells using NumPy indexing\n",
    "        image[::cell_size, :] = (0, 0, 0)\n",
    "        image[:, ::cell_size] = (0, 0, 0)\n",
    "        image[::cell_size, -1] = (0, 0, 0)\n",
    "        image[-1, ::cell_size] = (0, 0, 0)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            frame = self._generate_frame(self.current_state)\n",
    "            cv2.imshow('Hide & Seek', frame)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            print(self.current_state)\n",
    "        \n",
    "    def get_best_seeker_action(self):\n",
    "        cache = self.cache['best_seeker_action']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1], self.hider_pos[0], self.hider_pos[1])\n",
    "            if cache[key]!=-1:\n",
    "                return cache[key]\n",
    "\n",
    "        best_action = self._compute_best_seeker_action(self.walls, self.seeker_pos, self.hider_pos)\n",
    "        if self.use_cache:\n",
    "            cache[key] = best_action\n",
    "\n",
    "        return best_action\n",
    "    \n",
    "    def _compute_best_seeker_action(self, walls, seeker_pos, hider_pos):\n",
    "        grid = Grid(matrix=~walls)\n",
    "        start = grid.node(seeker_pos[1], seeker_pos[0])\n",
    "        end = grid.node(hider_pos[1], hider_pos[0])\n",
    "        finder = DijkstraFinder()\n",
    "        path, runs = finder.find_path(start, end, grid)\n",
    "        if len(path) == 0: # no path found\n",
    "            return STAY\n",
    "        next_cell = path[-len(path)+1]\n",
    "        best_move = (next_cell[1] - seeker_pos[0], next_cell[0] - seeker_pos[1])\n",
    "        best_action = self._move_to_action(best_move)\n",
    "        if best_action == STAY:\n",
    "            print(\"Seeker best action is to stay ?!\")\n",
    "        return best_action\n",
    "\n",
    "    def _move_to_action(self, move):\n",
    "        if move == (-1, 0):\n",
    "            action = UP\n",
    "        elif move == (1, 0):\n",
    "            action = DOWN\n",
    "        elif move == (0, -1):\n",
    "            action = LEFT\n",
    "        elif move == (0, 1):\n",
    "            action = RIGHT\n",
    "        elif move == (0, 0):\n",
    "            action = STAY\n",
    "        return action\n",
    "\n",
    "    def _move_seeker(self):\n",
    "        # best_action = self.get_best_seeker_action()\n",
    "        # prob = self.current_step / self.max_steps\n",
    "        prob = 0.2\n",
    "        # if self.current_step < 10:\n",
    "        #     return\n",
    "        if np.random.binomial(1, prob):\n",
    "            action = self.get_best_seeker_action()\n",
    "        else:\n",
    "            valid_actions = self._get_valid_actions(self.seeker_pos)\n",
    "            action = random.choice(valid_actions)\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "\n",
    "        \n",
    "env = Monitor(HideAndSeekEnv(\n",
    "    grid_size=12,\n",
    "    vision_range=5,\n",
    "    seq_len=1,\n",
    "    render_mode=\"human\",\n",
    "    use_cache=True,\n",
    "    max_steps=300,\n",
    "    # start_movement_at=100_000,\n",
    "    # start_opt_movement_at=10,\n",
    "))\n",
    "# env.render()\n",
    "# env.step(UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d81f77d4ed941c8ae9073b6da40dfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\xampp\\htdocs\\FDS\\projets\\jobs tests\\DeepQLearning-Hide-and-Seek\\src\\env.ipynb Cell 4\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m log_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             learning_rate\u001b[39m=\u001b[39mlr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             exploration_final_eps\u001b[39m=\u001b[39mexploration,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             tensorboard_log\u001b[39m=\u001b[39mlog_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m500_000\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/xampp/htdocs/FDS/projets/jobs%20tests/DeepQLearning-Hide-and-Seek/src/env.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    268\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    269\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    270\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    272\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    273\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    274\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:331\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    330\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 331\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    333\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:206\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    203\u001b[0m     target_q_values \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mrewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m replay_data\u001b[39m.\u001b[39mdones) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q_values\n\u001b[0;32m    205\u001b[0m \u001b[39m# Get current Q-values estimates\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m current_q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net(replay_data\u001b[39m.\u001b[39;49mobservations)\n\u001b[0;32m    208\u001b[0m \u001b[39m# Retrieve the q-values for the actions from the replay buffer\u001b[39;00m\n\u001b[0;32m    209\u001b[0m current_q_values \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mgather(current_q_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mreplay_data\u001b[39m.\u001b[39mactions\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     60\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m    Predict the q-values.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m    :param obs: Observation\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor))\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Marwa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "lr = 1e-4\n",
    "exploration = 0.1\n",
    "log_dir = \"logs\"\n",
    "model = DQN(\"MlpPolicy\", env, \n",
    "            verbose=0,\n",
    "            learning_rate=lr,\n",
    "            exploration_final_eps=exploration,\n",
    "            tensorboard_log=log_dir,\n",
    "            )\n",
    "model.learn(\n",
    "    total_timesteps=500_000, \n",
    "    progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/mlp_20_100_1_prob=0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_episodes = 1000\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "        model,\n",
    "        env,\n",
    "        n_eval_episodes=nb_episodes\n",
    "    )\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"prob_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a given map, we have walls, visible cells, paths\n",
    "we have a class map, that loads the map from a json file and provide all those attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "# for i in range(1000):\n",
    "#     action, _state = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = vec_env.step(action)\n",
    "#     vec_env.render(\"human\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# vec_env = model.get_env()\n",
    "# obs = vec_env.reset()\n",
    "vec_env.render(\"human\")\n",
    "action, _state = model.predict(obs, deterministic=True)\n",
    "obs, reward, done, info = vec_env.step(action)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "STAY = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hider was caught\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0],\n",
       "        [1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0],\n",
       "        [1, 1, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0],\n",
       "        [1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 0, 0],\n",
       "        [1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 0],\n",
       "        [0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0],\n",
       "        [0, 0, 2, 2, 2, 0, 0, 2, 2, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8),\n",
       " -100,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(UP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 600, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define the colors using RGB values\n",
    "colors = np.array([(255, 255, 255),  # Color 0\n",
    "                   (255, 0, 0),      # Color 1\n",
    "                   (0, 255, 0),      # Color 2\n",
    "                   (0, 0, 255),      # Color 3\n",
    "                   (255, 255, 0)],   # Color 4\n",
    "                  dtype=np.uint8)\n",
    "\n",
    "# Create a matrix representing the grid\n",
    "matrix = np.random.randint(low=0, high=5, size=(12, 12), dtype=np.uint8)  # Random color indices for demonstration\n",
    "\n",
    "# Define cell size and line thickness for display\n",
    "cell_size = 50\n",
    "\n",
    "# Calculate image size based on grid dimensions and cell size\n",
    "image_size = (matrix.shape[1] * cell_size, matrix.shape[0] * cell_size)\n",
    "\n",
    "# Create a blank canvas with white background\n",
    "image = np.ones((matrix.shape[1], matrix.shape[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "# Fill each cell with the corresponding color using NumPy indexing\n",
    "image_rows = np.arange(matrix.shape[0]) * cell_size\n",
    "image_cols = np.arange(matrix.shape[1]) * cell_size\n",
    "\n",
    "# print(image[0, :60])\n",
    "image = colors[matrix]\n",
    "# repeat each row cell_size times\n",
    "image = np.repeat(image, cell_size, axis=0)\n",
    "image = np.repeat(image, cell_size, axis=1)\n",
    "\n",
    "# image[image_rows[:, np.newaxis], image_cols] = colors[matrix]\n",
    "print(image.shape)\n",
    "# print(image[0, :60])\n",
    "# print(colors[matrix].shape)\n",
    "\n",
    "# compare old and new image\n",
    "\n",
    "# Draw black lines as separators between cells using NumPy indexing\n",
    "image[::cell_size, :] = (0, 0, 0)\n",
    "image[:, ::cell_size] = (0, 0, 0)\n",
    "image[::cell_size, -1] = (0, 0, 0)\n",
    "image[-1, ::cell_size] = (0, 0, 0)\n",
    "\n",
    "\n",
    "\n",
    "# print(image)\n",
    "# Display the imageq\n",
    "cv2.imshow('Grid Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances from point 'a' to each cell:\n",
      "[[ 5.38516481  4.47213595  3.60555128  2.82842712  2.23606798  2.\n",
      "   2.23606798  2.82842712  3.60555128  4.47213595  5.38516481  6.32455532]\n",
      " [ 5.09901951  4.12310563  3.16227766  2.23606798  1.41421356  1.\n",
      "   1.41421356  2.23606798  3.16227766  4.12310563  5.09901951  6.08276253]\n",
      " [ 5.          4.          3.          2.          1.          0.\n",
      "   1.          2.          3.          4.          5.          6.        ]\n",
      " [ 5.09901951  4.12310563  3.16227766  2.23606798  1.41421356  1.\n",
      "   1.41421356  2.23606798  3.16227766  4.12310563  5.09901951  6.08276253]\n",
      " [ 5.38516481  4.47213595  3.60555128  2.82842712  2.23606798  2.\n",
      "   2.23606798  2.82842712  3.60555128  4.47213595  5.38516481  6.32455532]\n",
      " [ 5.83095189  5.          4.24264069  3.60555128  3.16227766  3.\n",
      "   3.16227766  3.60555128  4.24264069  5.          5.83095189  6.70820393]\n",
      " [ 6.40312424  5.65685425  5.          4.47213595  4.12310563  4.\n",
      "   4.12310563  4.47213595  5.          5.65685425  6.40312424  7.21110255]\n",
      " [ 7.07106781  6.40312424  5.83095189  5.38516481  5.09901951  5.\n",
      "   5.09901951  5.38516481  5.83095189  6.40312424  7.07106781  7.81024968]\n",
      " [ 7.81024968  7.21110255  6.70820393  6.32455532  6.08276253  6.\n",
      "   6.08276253  6.32455532  6.70820393  7.21110255  7.81024968  8.48528137]\n",
      " [ 8.60232527  8.06225775  7.61577311  7.28010989  7.07106781  7.\n",
      "   7.07106781  7.28010989  7.61577311  8.06225775  8.60232527  9.21954446]\n",
      " [ 9.43398113  8.94427191  8.54400375  8.24621125  8.06225775  8.\n",
      "   8.06225775  8.24621125  8.54400375  8.94427191  9.43398113 10.        ]\n",
      " [10.29563014  9.8488578   9.48683298  9.21954446  9.05538514  9.\n",
      "   9.05538514  9.21954446  9.48683298  9.8488578  10.29563014 10.81665383]]\n"
     ]
    }
   ],
   "source": [
    "matrix = np.zeros((12, 12))\n",
    "\n",
    "# Define the coordinates of point \"a\"\n",
    "a = (2, 5)  # Row, Column\n",
    "\n",
    "# Compute the distance between point \"a\" and each cell in the matrix\n",
    "distances = np.linalg.norm(np.indices(matrix.shape) - np.array(a)[:, np.newaxis, np.newaxis], axis=0)\n",
    "\n",
    "print(\"Distances from point 'a' to each cell:\")\n",
    "print(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENTS OPENAI GYM\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "\n",
    "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
    "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
    "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
    "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
    "     in the left and right direction on the cart.\n",
    "\n",
    "    ### Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
    "     of the fixed force the cart is pushed with.\n",
    "\n",
    "    | Num | Action                 |\n",
    "    |-----|------------------------|\n",
    "    | 0   | Push cart to the left  |\n",
    "    | 1   | Push cart to the right |\n",
    "\n",
    "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
    "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
    "\n",
    "    ### Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(4,)` with the values corresponding to the following positions and velocities:\n",
    "\n",
    "    | Num | Observation           | Min                 | Max               |\n",
    "    |-----|-----------------------|---------------------|-------------------|\n",
    "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
    "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
    "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
    "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
    "\n",
    "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
    "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
    "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
    "       if the cart leaves the `(-2.4, 2.4)` range.\n",
    "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
    "       if the pole angle is not in the range `(-.2095, .2095)` (or **±12°**)\n",
    "\n",
    "    ### Rewards\n",
    "\n",
    "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
    "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
    "\n",
    "    ### Starting State\n",
    "\n",
    "    All observations are assigned a uniformly random value in `(-0.05, 0.05)`\n",
    "\n",
    "    ### Episode End\n",
    "\n",
    "    The episode ends if any one of the following occurs:\n",
    "\n",
    "    1. Termination: Pole Angle is greater than ±12°\n",
    "    2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
    "    3. Truncation: Episode length is greater than 500 (200 for v0)\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('CartPole-v1')\n",
    "    ```\n",
    "\n",
    "    No additional arguments are currently supported.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot**2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not terminated:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_terminated is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_terminated = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_terminated == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned terminated = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_terminated += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(\n",
    "            options, -0.05, 0.05  # default low\n",
    "        )  # default high\n",
    "        self.state = self.np_random.uniform(low=low, high=high, size=(4,))\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 100  # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n",
    "\n",
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "\n",
    "\n",
    "class Continuous_MountainCarEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    ### Description\n",
    "\n",
    "    The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically\n",
    "    at the bottom of a sinusoidal valley, with the only possible actions being the accelerations\n",
    "    that can be applied to the car in either direction. The goal of the MDP is to strategically\n",
    "    accelerate the car to reach the goal state on top of the right hill. There are two versions\n",
    "    of the mountain car domain in gym: one with discrete actions and one with continuous.\n",
    "    This version is the one with continuous actions.\n",
    "\n",
    "    This MDP first appeared in [Andrew Moore's PhD Thesis (1990)](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)\n",
    "\n",
    "    ```\n",
    "    @TECHREPORT{Moore90efficientmemory-based,\n",
    "        author = {Andrew William Moore},\n",
    "        title = {Efficient Memory-based Learning for Robot Control},\n",
    "        institution = {University of Cambridge},\n",
    "        year = {1990}\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ### Observation Space\n",
    "\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "\n",
    "    | Num | Observation                          | Min  | Max | Unit         |\n",
    "    |-----|--------------------------------------|------|-----|--------------|\n",
    "    | 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
    "    | 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
    "\n",
    "    ### Action Space\n",
    "\n",
    "    The action is a `ndarray` with shape `(1,)`, representing the directional force applied on the car.\n",
    "    The action is clipped in the range `[-1,1]` and multiplied by a power of 0.0015.\n",
    "\n",
    "    ### Transition Dynamics:\n",
    "\n",
    "    Given an action, the mountain car follows the following transition dynamics:\n",
    "\n",
    "    *velocity<sub>t+1</sub> = velocity<sub>t+1</sub> + force * self.power - 0.0025 * cos(3 * position<sub>t</sub>)*\n",
    "\n",
    "    *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
    "\n",
    "    where force is the action clipped to the range `[-1,1]` and power is a constant 0.0015.\n",
    "    The collisions at either end are inelastic with the velocity set to 0 upon collision with the wall.\n",
    "    The position is clipped to the range [-1.2, 0.6] and velocity is clipped to the range [-0.07, 0.07].\n",
    "\n",
    "    ### Reward\n",
    "\n",
    "    A negative reward of *-0.1 * action<sup>2</sup>* is received at each timestep to penalise for\n",
    "    taking actions of large magnitude. If the mountain car reaches the goal then a positive reward of +100\n",
    "    is added to the negative reward for that timestep.\n",
    "\n",
    "    ### Starting State\n",
    "\n",
    "    The position of the car is assigned a uniform random value in `[-0.6 , -0.4]`.\n",
    "    The starting velocity of the car is always assigned to 0.\n",
    "\n",
    "    ### Episode End\n",
    "\n",
    "    The episode ends if either of the following happens:\n",
    "    1. Termination: The position of the car is greater than or equal to 0.45 (the goal position on top of the right hill)\n",
    "    2. Truncation: The length of the episode is 999.\n",
    "\n",
    "    ### Arguments\n",
    "\n",
    "    ```\n",
    "    gym.make('MountainCarContinuous-v0')\n",
    "    ```\n",
    "\n",
    "    ### Version History\n",
    "\n",
    "    * v0: Initial versions release (1.0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None, goal_velocity=0):\n",
    "        self.min_action = -1.0\n",
    "        self.max_action = 1.0\n",
    "        self.min_position = -1.2\n",
    "        self.max_position = 0.6\n",
    "        self.max_speed = 0.07\n",
    "        self.goal_position = (\n",
    "            0.45  # was 0.5 in gym, 0.45 in Arnaud de Broissia's version\n",
    "        )\n",
    "        self.goal_velocity = goal_velocity\n",
    "        self.power = 0.0015\n",
    "\n",
    "        self.low_state = np.array(\n",
    "            [self.min_position, -self.max_speed], dtype=np.float32\n",
    "        )\n",
    "        self.high_state = np.array(\n",
    "            [self.max_position, self.max_speed], dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.screen_width = 600\n",
    "        self.screen_height = 400\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "            low=self.min_action, high=self.max_action, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=self.low_state, high=self.high_state, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def step(self, action: np.ndarray):\n",
    "\n",
    "        position = self.state[0]\n",
    "        velocity = self.state[1]\n",
    "        force = min(max(action[0], self.min_action), self.max_action)\n",
    "\n",
    "        velocity += force * self.power - 0.0025 * math.cos(3 * position)\n",
    "        if velocity > self.max_speed:\n",
    "            velocity = self.max_speed\n",
    "        if velocity < -self.max_speed:\n",
    "            velocity = -self.max_speed\n",
    "        position += velocity\n",
    "        if position > self.max_position:\n",
    "            position = self.max_position\n",
    "        if position < self.min_position:\n",
    "            position = self.min_position\n",
    "        if position == self.min_position and velocity < 0:\n",
    "            velocity = 0\n",
    "\n",
    "        # Convert a possible numpy bool to a Python bool.\n",
    "        terminated = bool(\n",
    "            position >= self.goal_position and velocity >= self.goal_velocity\n",
    "        )\n",
    "\n",
    "        reward = 0\n",
    "        if terminated:\n",
    "            reward = 100.0\n",
    "        reward -= math.pow(action[0], 2) * 0.1\n",
    "\n",
    "        self.state = np.array([position, velocity], dtype=np.float32)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return self.state, reward, terminated, False, {}\n",
    "\n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        super().reset(seed=seed)\n",
    "        # Note that if you use custom reset bounds, it may lead to out-of-bound\n",
    "        # state/observations.\n",
    "        low, high = utils.maybe_parse_reset_bounds(options, -0.6, -0.4)\n",
    "        self.state = np.array([self.np_random.uniform(low=low, high=high), 0])\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def _height(self, xs):\n",
    "        return np.sin(3 * xs) * 0.45 + 0.55\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\":\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.max_position - self.min_position\n",
    "        scale = self.screen_width / world_width\n",
    "        carwidth = 40\n",
    "        carheight = 20\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        pos = self.state[0]\n",
    "\n",
    "        xs = np.linspace(self.min_position, self.max_position, 100)\n",
    "        ys = self._height(xs)\n",
    "        xys = list(zip((xs - self.min_position) * scale, ys * scale))\n",
    "\n",
    "        pygame.draw.aalines(self.surf, points=xys, closed=False, color=(0, 0, 0))\n",
    "\n",
    "        clearance = 10\n",
    "\n",
    "        l, r, t, b = -carwidth / 2, carwidth / 2, carheight, 0\n",
    "        coords = []\n",
    "        for c in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(math.cos(3 * pos))\n",
    "            coords.append(\n",
    "                (\n",
    "                    c[0] + (pos - self.min_position) * scale,\n",
    "                    c[1] + clearance + self._height(pos) * scale,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        gfxdraw.aapolygon(self.surf, coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, coords, (0, 0, 0))\n",
    "\n",
    "        for c in [(carwidth / 4, 0), (-carwidth / 4, 0)]:\n",
    "            c = pygame.math.Vector2(c).rotate_rad(math.cos(3 * pos))\n",
    "            wheel = (\n",
    "                int(c[0] + (pos - self.min_position) * scale),\n",
    "                int(c[1] + clearance + self._height(pos) * scale),\n",
    "            )\n",
    "\n",
    "            gfxdraw.aacircle(\n",
    "                self.surf, wheel[0], wheel[1], int(carheight / 2.5), (128, 128, 128)\n",
    "            )\n",
    "            gfxdraw.filled_circle(\n",
    "                self.surf, wheel[0], wheel[1], int(carheight / 2.5), (128, 128, 128)\n",
    "            )\n",
    "\n",
    "        flagx = int((self.goal_position - self.min_position) * scale)\n",
    "        flagy1 = int(self._height(self.goal_position) * scale)\n",
    "        flagy2 = flagy1 + 50\n",
    "        gfxdraw.vline(self.surf, flagx, flagy1, flagy2, (0, 0, 0))\n",
    "\n",
    "        gfxdraw.aapolygon(\n",
    "            self.surf,\n",
    "            [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)],\n",
    "            (204, 204, 0),\n",
    "        )\n",
    "        gfxdraw.filled_polygon(\n",
    "            self.surf,\n",
    "            [(flagx, flagy2), (flagx, flagy2 - 10), (flagx + 25, flagy2 - 5)],\n",
    "            (204, 204, 0),\n",
    "        )\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from vision import compute_visible_cells\n",
    "from helpers import default_map\n",
    "import cv2\n",
    "import time\n",
    "import math\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from pathfinding.core.diagonal_movement import DiagonalMovement\n",
    "from pathfinding.core.grid import Grid\n",
    "from pathfinding.finder.a_star import AStarFinder\n",
    "from pathfinding.finder.dijkstra import DijkstraFinder\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import random\n",
    "\n",
    "GREEN = (0, 255, 0)\n",
    "RED = (0, 0, 255)\n",
    "BLUE = (255, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "YELLOW = (0, 255, 255)\n",
    "\n",
    "colors = np.array([WHITE,  # hidden cells\n",
    "                   YELLOW, # visible cells\n",
    "                   BLACK,  # walls\n",
    "                   RED,  # seeker\n",
    "                   GREEN],   # hider\n",
    "                  dtype=np.uint8)\n",
    "\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "STAY = 4\n",
    "\n",
    "HITTING_WALL_REWARD = -1\n",
    "LOSE_REWARD = 0\n",
    "DISTANCE_COEF_REWARD = 5\n",
    "\n",
    "MIN_RES = 36\n",
    "\n",
    "class HideAndSeekEnv(gym.Env):\n",
    "    def __init__(self, \n",
    "                 grid_size=12, \n",
    "                 vision_range=5, \n",
    "                 seq_len=5,\n",
    "                 use_cache=True, \n",
    "                 max_steps=None,\n",
    "                 render_mode='rgb_array',\n",
    "                 fps=5,\n",
    "                ):\n",
    "        super(HideAndSeekEnv, self).__init__()\n",
    "\n",
    "        assert vision_range > 0, \"Vision range must be greater than 0\"\n",
    "        assert grid_size > 0, \"Grid size must be greater than 0\"\n",
    "        assert max_steps is None or max_steps > 0, \"Max steps must be greater than 0\" \n",
    "        assert render_mode in ['rgb_array', 'human'], \"Render mode must be 'rgb_array' or 'human'\"\n",
    "        assert seq_len > 0, \"Sequence length must be greater than 0\"\n",
    "        \n",
    "        if max_steps is None:\n",
    "            max_steps = np.inf\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.vision_range = vision_range\n",
    "        self.seq_len = seq_len\n",
    "        self.render_mode = render_mode\n",
    "        self.use_cache = use_cache\n",
    "        self.fps = fps\n",
    "\n",
    "        self.seeker_pos = None\n",
    "        self.hider_pos = None\n",
    "        self.cache = {\n",
    "            'best_seeker_action':-1*np.ones(\n",
    "                (self.grid_size, self.grid_size, self.grid_size, self.grid_size), \n",
    "                dtype=int,\n",
    "            ),\n",
    "            'visible_cells':{},\n",
    "        }\n",
    "        self.upscale_factor = math.ceil(MIN_RES/self.grid_size)\n",
    "\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right, Stay\n",
    "        self.obs_dict = {\n",
    "            \"hidden\":0,\n",
    "            \"visible\":1,\n",
    "            \"wall\":2,\n",
    "            \"seeker\":3,\n",
    "            \"hider\":4\n",
    "        }\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.grid_size*self.upscale_factor, self.grid_size*self.upscale_factor, self.seq_len),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        self.current_state = None\n",
    "        self.current_step = 0\n",
    "        self.prev_states = None\n",
    "        self.walls = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "        self.visible_cells = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
    "\n",
    "        self.mode = None\n",
    "        self.train()\n",
    "        # check_env(self)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def train(self):\n",
    "        self.mode = 'train'\n",
    "    \n",
    "    def eval(self):\n",
    "        self.mode = 'eval'\n",
    "    \n",
    "    def test(self):\n",
    "        self.mode = 'test'\n",
    "\n",
    "    def get_processed_state(self):\n",
    "        processed_state = self.prev_states[:, :, -self.seq_len:]\n",
    "        processed_state = np.repeat(processed_state, self.upscale_factor, axis=0)\n",
    "        processed_state = np.repeat(processed_state, self.upscale_factor, axis=1)\n",
    "        return processed_state\n",
    "\n",
    "    def update_current_state(self):\n",
    "        self.current_state = np.full(\n",
    "            shape=(self.grid_size, self.grid_size), \n",
    "            fill_value=self.obs_dict['hidden'], \n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.current_state[self.walls] = self.obs_dict['wall']\n",
    "        self.current_state[self.visible_cells] = self.obs_dict['visible']\n",
    "        self.current_state[self.seeker_pos[0], self.seeker_pos[1]] = self.obs_dict['seeker']\n",
    "        self.current_state[self.hider_pos[0], self.hider_pos[1]] = self.obs_dict['hider']\n",
    "        \n",
    "        self.prev_states = np.append(\n",
    "            self.prev_states, \n",
    "            np.expand_dims(self.current_state, axis=-1), \n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None,):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.prev_states = np.empty((self.grid_size, self.grid_size, 0), dtype=np.uint8)\n",
    "\n",
    "        self.walls = self._generate_walls()\n",
    "        self.seeker_pos = self.generate_seeker_pos()\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "        self.hider_pos = self.generate_hider_pos()\n",
    "\n",
    "        for _ in range(self.seq_len):\n",
    "            self.update_current_state()\n",
    "\n",
    "        return self.get_processed_state(), self._get_info()\n",
    "\n",
    "    def get_visible_cells(self):\n",
    "        cache = self.cache['visible_cells']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1])\n",
    "            if key in cache:\n",
    "                return cache[key]\n",
    "\n",
    "        visible_cells = compute_visible_cells(self.walls, self.seeker_pos, self.vision_range)\n",
    "        if self.use_cache:\n",
    "            cache[key] = visible_cells\n",
    "\n",
    "        return visible_cells\n",
    "\n",
    "    def _generate_walls(self):\n",
    "        walls_mask = default_map()\n",
    "        return walls_mask\n",
    "        \n",
    "    def generate_hider_pos(self):\n",
    "        allowed_cells = ~(self.walls | self.visible_cells)\n",
    "        allowed_cells[self.seeker_pos[0], self.seeker_pos[1]] = False\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "\n",
    "    def generate_seeker_pos(self):\n",
    "        allowed_cells = ~self.walls\n",
    "        return self.sample_from_allowed_cells(allowed_cells)\n",
    "    \n",
    "    def index_to_coords(self, index):\n",
    "        return index // self.grid_size, index % self.grid_size\n",
    "    \n",
    "    def coords_to_index(self, coords):\n",
    "        return coords[0] * self.grid_size + coords[1]\n",
    "    \n",
    "    def sample_from_allowed_cells(self, allowed_cells):\n",
    "        prob = allowed_cells.astype(np.float32)\n",
    "        prob /= prob.sum()\n",
    "        flat_prob = prob.flatten()\n",
    "        sample_index = np.random.choice(flat_prob.size, p=flat_prob)\n",
    "        x, y = self.index_to_coords(sample_index)\n",
    "        return np.array([x, y], dtype=np.uint8)\n",
    "    \n",
    "    def _get_valid_actions(self, position):\n",
    "        valid_actions = []\n",
    "        if position[0] > 0 and not self.walls[position[0] - 1, position[1]]:\n",
    "            valid_actions.append(UP)\n",
    "        if position[0] < self.grid_size - 1 and not self.walls[position[0] + 1, position[1]]:\n",
    "            valid_actions.append(DOWN)\n",
    "        if position[1] > 0 and not self.walls[position[0], position[1] - 1]:\n",
    "            valid_actions.append(LEFT)\n",
    "        if position[1] < self.grid_size - 1 and not self.walls[position[0], position[1] + 1]:\n",
    "            valid_actions.append(RIGHT)\n",
    "        # valid_actions.append(STAY)\n",
    "        return valid_actions\n",
    "    \n",
    "    def _move(self, position, action):\n",
    "        if action == UP:  # Up\n",
    "            position[0] -= 1\n",
    "        elif action == DOWN:  # Down\n",
    "            position[0] += 1\n",
    "        elif action == LEFT:  # Left\n",
    "            position[1] -= 1\n",
    "        elif action == RIGHT:  # Right\n",
    "            position[1] += 1\n",
    "        elif action == STAY:  # Stay\n",
    "            print(\"Agent action: STAY ?!\")\n",
    "            pass\n",
    "        return position\n",
    "    \n",
    "    def _get_min_distance_from_visible_cells(self, position):\n",
    "        distances = np.linalg.norm(np.indices((self.grid_size, self.grid_size)) - position[:, np.newaxis, np.newaxis], axis=0)\n",
    "        distances[~self.visible_cells] = np.inf\n",
    "        return distances.min()\n",
    "    \n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "    \n",
    "    def move_player(self, action):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "        assert self.mode in ['test'], \"Call move_player only in test mode\"\n",
    "\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "    \n",
    "    def step(self, action, verbose=False):\n",
    "        assert self.action_space.contains(action), f\"{action} is an invalid action\"\n",
    "        assert self.current_state is not None, \"Call reset before using step method.\"\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        reward = 0\n",
    "        reward_log = {}\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            # seeker makes a move based on the old state\n",
    "            self._move_seeker()\n",
    "\n",
    "        valid_actions = self._get_valid_actions(self.hider_pos)\n",
    "        if action in valid_actions:\n",
    "            self._move(self.hider_pos, action)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Hider hit the wall\")\n",
    "            reward += HITTING_WALL_REWARD\n",
    "            reward_log['hitting_wall'] = HITTING_WALL_REWARD\n",
    "\n",
    "        min_distance = self._get_min_distance_from_visible_cells(self.hider_pos)\n",
    "        distance_reward = int(min_distance) * DISTANCE_COEF_REWARD\n",
    "        reward += distance_reward\n",
    "        reward_log['distance'] = distance_reward\n",
    "\n",
    "        terminated = False\n",
    "        if self.visible_cells[self.hider_pos[0], self.hider_pos[1]]:\n",
    "            if verbose:\n",
    "                print(\"Hider was caught\")\n",
    "            reward += LOSE_REWARD\n",
    "            reward_log['lose'] = LOSE_REWARD\n",
    "            terminated = True\n",
    "\n",
    "        truncated = self.current_step >= self.max_steps and not terminated\n",
    "        self.update_current_state()\n",
    "\n",
    "        return self.get_processed_state(), reward, terminated, truncated, self._get_info()\n",
    "\n",
    "    def _generate_frame(self, matrix, cell_size=50):\n",
    "        # Calculate image size based on grid dimensions and cell size\n",
    "        image_size = (matrix.shape[1] * cell_size, matrix.shape[0] * cell_size)\n",
    "\n",
    "        # Create a blank canvas with white background\n",
    "        image = np.ones((matrix.shape[1], matrix.shape[0], 3), dtype=np.uint8) * 255\n",
    "\n",
    "        # Fill each cell with the corresponding color using NumPy indexing\n",
    "        image_rows = np.arange(matrix.shape[0]) * cell_size\n",
    "        image_cols = np.arange(matrix.shape[1]) * cell_size\n",
    "\n",
    "        # print(image[0, :60])\n",
    "        image = colors[matrix]\n",
    "        # repeat each row cell_size times\n",
    "        image = np.repeat(image, cell_size, axis=0)\n",
    "        image = np.repeat(image, cell_size, axis=1)\n",
    "\n",
    "        # Draw black lines as separators between cells using NumPy indexing\n",
    "        image[::cell_size, :] = (0, 0, 0)\n",
    "        image[:, ::cell_size] = (0, 0, 0)\n",
    "        image[::cell_size, -1] = (0, 0, 0)\n",
    "        image[-1, ::cell_size] = (0, 0, 0)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            frame = self._generate_frame(self.current_state)\n",
    "            cv2.imshow('Hide & Seek', frame)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            print(self.current_state)\n",
    "        \n",
    "    def get_best_seeker_action(self):\n",
    "        cache = self.cache['best_seeker_action']\n",
    "        if self.use_cache:\n",
    "            key = (self.seeker_pos[0], self.seeker_pos[1], self.hider_pos[0], self.hider_pos[1])\n",
    "            if cache[key]!=-1:\n",
    "                return cache[key]\n",
    "\n",
    "        best_action = self._compute_best_seeker_action(self.walls, self.seeker_pos, self.hider_pos)\n",
    "        if self.use_cache:\n",
    "            cache[key] = best_action\n",
    "\n",
    "        return best_action\n",
    "    \n",
    "    def _compute_best_seeker_action(self, walls, seeker_pos, hider_pos):\n",
    "        grid = Grid(matrix=~walls)\n",
    "        start = grid.node(seeker_pos[1], seeker_pos[0])\n",
    "        end = grid.node(hider_pos[1], hider_pos[0])\n",
    "        finder = DijkstraFinder()\n",
    "        path, runs = finder.find_path(start, end, grid)\n",
    "        if len(path) == 0: # no path found\n",
    "            return STAY\n",
    "        next_cell = path[-len(path)+1]\n",
    "        best_move = (next_cell[1] - seeker_pos[0], next_cell[0] - seeker_pos[1])\n",
    "        best_action = self._move_to_action(best_move)\n",
    "        if best_action == STAY:\n",
    "            print(\"Seeker best action is to stay ?!\")\n",
    "        return best_action\n",
    "\n",
    "    def _move_to_action(self, move):\n",
    "        if move == (-1, 0):\n",
    "            action = UP\n",
    "        elif move == (1, 0):\n",
    "            action = DOWN\n",
    "        elif move == (0, -1):\n",
    "            action = LEFT\n",
    "        elif move == (0, 1):\n",
    "            action = RIGHT\n",
    "        elif move == (0, 0):\n",
    "            action = STAY\n",
    "        return action\n",
    "\n",
    "    def _move_seeker(self):\n",
    "        # best_action = self.get_best_seeker_action()\n",
    "        # prob = self.current_step / self.max_steps\n",
    "        prob = 0\n",
    "        return\n",
    "        if self.current_step < 10:\n",
    "            return\n",
    "        if np.random.binomial(1, prob):\n",
    "            action = self.get_best_seeker_action()\n",
    "        else:\n",
    "            valid_actions = self._get_valid_actions(self.seeker_pos)\n",
    "            action = random.choice(valid_actions)\n",
    "        self._move(self.seeker_pos, action)\n",
    "        self.visible_cells = self.get_visible_cells()\n",
    "\n",
    "        \n",
    "env = Monitor(HideAndSeekEnv(\n",
    "    grid_size=12,\n",
    "    vision_range=5,\n",
    "    seq_len=1,\n",
    "    render_mode=\"human\",\n",
    "    use_cache=True,\n",
    "    max_steps=300,\n",
    "))\n",
    "# env.render()\n",
    "# env.step(UP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cb7d321a1544a2b81c3c3d8508981d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "# env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "lr = 1e-4\n",
    "exploration = 0.1\n",
    "log_dir = \"logs\"\n",
    "model = DQN(\"CnnPolicy\", env, \n",
    "            verbose=0,\n",
    "            learning_rate=lr,\n",
    "            exploration_final_eps=exploration,\n",
    "            tensorboard_log=log_dir,\n",
    "            )\n",
    "model.learn(\n",
    "    total_timesteps=500_000, \n",
    "    progress_bar=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 1), (2, 1)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bresenham(x0, y0, x1, y1):\n",
    "    \"\"\"Yield integer coordinates on the line from (x0, y0) to (x1, y1).\n",
    "\n",
    "    Input coordinates should be integers.\n",
    "\n",
    "    The result will contain both the start and the end point.\n",
    "    \"\"\"\n",
    "    dx = x1 - x0\n",
    "    dy = y1 - y0\n",
    "\n",
    "    xsign = 1 if dx > 0 else -1\n",
    "    ysign = 1 if dy > 0 else -1\n",
    "\n",
    "    dx = abs(dx)\n",
    "    dy = abs(dy)\n",
    "\n",
    "    if dx > dy:\n",
    "        xx, xy, yx, yy = xsign, 0, 0, ysign\n",
    "    else:\n",
    "        dx, dy = dy, dx\n",
    "        xx, xy, yx, yy = 0, ysign, xsign, 0\n",
    "\n",
    "    D = 2*dy - dx\n",
    "    y = 0\n",
    "    result = []\n",
    "    for x in range(dx + 1):\n",
    "        result.append((x0 + x * xx + y * yx, y0 + x * xy + y * yy))\n",
    "\n",
    "        if D >= 0:\n",
    "            y += 1\n",
    "            D -= 2*dx\n",
    "        D += 2*dy \n",
    "    return result\n",
    "\n",
    "bresenham(0, 0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visible_cells(self, walls, agent_position, distance):\n",
    "        grid_size = walls.shape[0]\n",
    "        visible_cells = np.zeros((grid_size, grid_size), dtype=bool)\n",
    "\n",
    "        x, y = agent_position\n",
    "\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if walls[i, j] == 0:\n",
    "                    dx = i - x\n",
    "                    dy = j - y\n",
    "\n",
    "                    if dx == 0 and dy == 0:\n",
    "                        continue\n",
    "\n",
    "                    magnitude = np.sqrt(dx**2 + dy**2)\n",
    "                    step_x = dx / magnitude\n",
    "                    step_y = dy / magnitude\n",
    "\n",
    "                    cur_x = x\n",
    "                    cur_y = y\n",
    "                    reached_limit = False\n",
    "\n",
    "                    for _ in range(int(distance)):\n",
    "                        cur_x += step_x\n",
    "                        cur_y += step_y\n",
    "\n",
    "                        if not (0 <= int(cur_x) < grid_size and 0 <= int(cur_y) < grid_size):\n",
    "                            reached_limit = True\n",
    "                            break\n",
    "\n",
    "                        if walls[int(cur_x), int(cur_y)] == 1:\n",
    "                            break\n",
    "\n",
    "                        visible_cells[int(cur_x), int(cur_y)] = True\n",
    "\n",
    "                    if reached_limit:\n",
    "                        break\n",
    "            visible_cells[x, y] = False\n",
    "\n",
    "        return visible_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class HideAndSeekEnv(gym.Env):\n",
    "    def __init__(self, grid_size=12):\n",
    "        super(HideAndSeekEnv, self).__init__()\n",
    "\n",
    "        self.grid_size = grid_size\n",
    "        self.num_steps = 20\n",
    "\n",
    "        self.seeker_pos = None\n",
    "        self.hider_pos = None\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.grid_size),\n",
    "            spaces.Discrete(self.grid_size),\n",
    "        ))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.seeker_pos = None\n",
    "        self.hider_pos = None\n",
    "\n",
    "        self._generate_walls()\n",
    "\n",
    "        while True:\n",
    "            seeker_x = np.random.randint(1, self.grid_size - 1)\n",
    "            seeker_y = np.random.randint(1, self.grid_size - 1)\n",
    "            if self._is_valid_position(seeker_x, seeker_y):\n",
    "                self.seeker_pos = (seeker_x, seeker_y)\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            hider_x = np.random.randint(1, self.grid_size - 1)\n",
    "            hider_y = np.random.randint(1, self.grid_size - 1)\n",
    "            if self._is_valid_position(hider_x, hider_y):\n",
    "                self.hider_pos = (hider_x, hider_y)\n",
    "                break\n",
    "\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in range(self.action_space.n), \"Invalid action.\"\n",
    "\n",
    "        new_pos = self._move_agent(self.seeker_pos, action)\n",
    "        if self._is_valid_position(*new_pos):\n",
    "            self.seeker_pos = new_pos\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = self.current_step >= self.num_steps\n",
    "        reward = self._calculate_reward()\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _generate_walls(self):\n",
    "        self.walls = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.walls[0, :] = 1\n",
    "        self.walls[-1, :] = 1\n",
    "        self.walls[:, 0] = 1\n",
    "        self.walls[:, -1] = 1\n",
    "\n",
    "        num_random_walls = int(self.grid_size * self.grid_size * 0.1)\n",
    "        random_wall_indices = np.random.choice(\n",
    "            np.arange(1, self.grid_size - 1),\n",
    "            size=(2, num_random_walls),\n",
    "            replace=False\n",
    "        )\n",
    "        self.walls[random_wall_indices] = 1\n",
    "\n",
    "    def _is_valid_position(self, x, y):\n",
    "        if self.walls[x, y] == 1:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.seeker_pos\n",
    "\n",
    "    def _move_agent(self, pos, action):\n",
    "        new_pos = list(pos)\n",
    "        if action == 0:  # Up\n",
    "            new_pos[1] = min(pos[1] + 1, self.grid_size - 1)\n",
    "        elif action == 1:  # Down\n",
    "            new_pos[1] = max(pos[1] - 1, 0)\n",
    "        elif action == 2:  # Left\n",
    "            new_pos[0] = max(pos[0] - 1, 0)\n",
    "        elif action == 3:  # Right\n",
    "            new_pos[0] = min(pos[0] + 1, self.grid_size - 1)\n",
    "        return tuple(new_pos)\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        if self.seeker_pos == self.hider_pos:\n",
    "            return 1.0\n",
    "        return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymnasium.envs.registration import (\n",
    "#     make,\n",
    "#     spec,\n",
    "#     register,\n",
    "#     registry,\n",
    "#     pprint_registry,\n",
    "#     make_vec,\n",
    "# )\n",
    "# register(\n",
    "#     id=\"CartPole-v0\",\n",
    "#     entry_point=\"gymnasium.envs.classic_control.cartpole:CartPoleEnv\",\n",
    "#     vector_entry_point=\"gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv\",\n",
    "#     max_episode_steps=200,\n",
    "#     reward_threshold=195.0,\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
